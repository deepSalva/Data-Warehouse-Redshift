# Cluster Data Warehouse For A Music Streaming App

<div align="justify">
This repository is a use case for developing a Redshift cluster data warehouse (DWH) in Amazon Web Service (AWS).
The goal of the repository is to perform an ETL process in the cloud environment. The use case is based on an
imaginary music streaming startup, Sparkify, that has grown their user base and song database and want to move
their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity
on the app, as well as a directory with JSON metadata on the songs in their app.


We are performing the data engineer role, so we are tasked with building an ETL pipeline that extracts their data
from S3, stages them in Redshift, and transforms data into a set of dimensional tables for their analytics team to
continue finding insights into what songs their users are listening to.


The project is based on the project: Data Warehouse in the Data Engineer course, Udacity.


Prerequisites:
* Python mid level
* AWS environment: Create an account, IAM, s3 and EC2 management basics. 
* Confortable with SQL basic operations for database creation. 
* Basic data modeling, 3rd Normal Form, denormaliztion, etc... will be a plus


## Project Datasets


We work with 3 datasets that reside in S3. Here are the S3 links for each:


* Song data: `s3://udacity-dend/song_data`
* Log data: `s3://udacity-dend/log_data`
* This third file `s3://udacity-dend/log_json_path.json` contains the meta information that is required by AWS to
correctly load `s3://udacity-dend/log_data`

The first dataset is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains 
metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's 
track ID. For example, here are file paths to two files in this dataset.

The second dataset consists of log files in JSON format generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the 
dataset above. These simulate app activity logs from an imaginary music streaming app based on configuration settings.

For loading JSON files to Redshift check [here](https://hevodata.com/learn/json-to-redshift/)

## ETL pipeline design


For this exercise we will create 2 schemas.
* First schema with 2 staging tables: *events_staging* and *songs_staging*  where we will load all the data from the s3
buckets
* Second a star schema with fact and dimension tables for data analytics:
  * Fact table: **songplays** - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
  * Dimension table:
     * **users**: user_id, first_name, last_name, gender, level
     * **artists**: artist_id, name, location, latitude, longitude
     * **songs**: song_id, title, artist_id, year, duration
     * **time**: start_time, hour, day, week, month, year, weekday


The datasets are in JSON format allocated in the s3 buckets described above. To take advantage of the Redshift
capabilities, first we will load the data from the s3 buckets to staging tables in the cluster (ELT process). Once we
have all data in the staging tables we will transform it and load it into the star schema.


## Redshift cluster


To work with this project you will need to have an account in AWS. The project contain a function to gather the
endpoint and key necessary to connect with the cluster. All the necessary information to create the client and connect
with AWS has to be configured in the `dwh.cfg` file.


If you are not familiar with the AWS please refer to: [Redshift tutorial](https://docs.aws.amazon.com/redshift/latest/dg/tutorials-redshift.html)


To create the Redshift and the EC2 client we are working with [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)


## Project execution


As commented above, first you need to set up an account in AWS with IAM permissions. From there take your *KEY* and
secret number and insert them in the `dwh.cfg` file, as well as your *USER* and *PSW*.

**DO NOT** share your private information with anyone. In case you update the project, remember to delete sensitive information 
from your AWS account. 


Once you have this information you can start running the script.


* First run from terminal:
`$ python create_tables.py`


If no errors occur, it will create the cluster, create the tables for both schemas and give you the *endpoint* and *rol*
value that you need to insert in the `dwh.cfg` file, once the script ended.


* Then you can run the ETL process:
`$ python etl.py`


It will load the data into both schemas and execute some queries for testing and analysis


* After you finished, remember to clear all the resources. Otherwise, you will be extra charge from AWS. for that, run the following script:
`$ python clear_resources.py`


**WARNING!! Do not execute the script above unless you are sure you want to terminate the cluster process. Once you run the
script the work will be lost.**


## Query examples


Some queries executes:

Query 1:

*What are the 3 artists more listened by female gender in the year 2018*



```
SELECT artists.name, time.year, count(*)as reproductions
FROM songplays
JOIN artists        on (artists.artist_id = songplays.artist_id)
JOIN time           on (time.start_time = songplays.start_time)
JOIN users          on (users.user_id = songplays.user_id)
WHERE users.gender = 'F'
GROUP BY (artists.name, time.year)
order by count(*) desc
LIMIT 3;
```
Result:

```
                              name  year  reproductions
0                    Dwight Yoakam  2018             36
1                 Carleen Anderson  2018             25
2  Working For A Nuclear Free City  2018             16


```

Query 2:

Most song title played in *Atlanta-Sandy Springs-Roswell, GA* 

```
SELECT songs.title, songplays.location, count(*) as reproductions
FROM  songplays
JOIN  songs        on (songs.song_id = songplays.song_id)
WHERE songplays.location = 'Atlanta-Sandy Springs-Roswell, GA'
GROUP BY (songs.title, songplays.location)
order by count(*) desc
LIMIT 1;
```

Resutl:

```
  title                           location  reproductions
0  Home  Atlanta-Sandy Springs-Roswell, GA              8
```




